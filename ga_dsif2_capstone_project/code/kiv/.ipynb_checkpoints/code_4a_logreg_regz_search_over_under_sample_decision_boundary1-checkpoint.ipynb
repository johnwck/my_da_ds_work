{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "541e0a45",
   "metadata": {},
   "source": [
    "<br>  \n",
    "\n",
    "## <span style='color:blue'>Section 1: Import</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bf7231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b3e9d",
   "metadata": {},
   "source": [
    "<br>  \n",
    "\n",
    "## <span style='color:blue'>Section 2: Read, drop columns, form X_train and y_train</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c2d65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------- read ----------\n",
    "\n",
    "df_1 = pd.read_csv('../data/data_train.csv')\n",
    "\n",
    "# ---------- drop ----------\n",
    "\n",
    "print('Before drop :', df_1.shape)\n",
    "df_1.drop(columns=['index',\n",
    "                   'pco2', 'ph', 'basophils', 'lactic_acid', 'bmi',\n",
    "                   'creatine_kinase', 'lymphocyte', 'neutrophils'], inplace=True)\n",
    "print('After drop:', df_1.shape)\n",
    "print('')\n",
    "\n",
    "# ---------- form X ----------\n",
    "\n",
    "X_train = df_1.drop(columns=['outcome'])\n",
    "print('X_train :', X_train.shape)\n",
    "\n",
    "# ---------- form y ----------\n",
    "\n",
    "y_train = df_1['outcome']\n",
    "print('y_train :', y_train.shape)\n",
    "print('y_train :', np.unique(y_train, return_counts=True))\n",
    "print('y_train :', Counter(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf22ab5",
   "metadata": {},
   "source": [
    "<br>  \n",
    "\n",
    "## <span style='color:blue'>Section 3: Function - Impute</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c50398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_fit_transform(to_impute, to_impute_fit_transform):\n",
    "    temp_df = []\n",
    "    temp_df = to_impute_fit_transform.fit_transform(to_impute)\n",
    "    temp_df = pd.DataFrame(temp_df, columns=to_impute.columns)\n",
    "    to_impute = copy.deepcopy(temp_df)\n",
    "    return to_impute, to_impute_fit_transform\n",
    "\n",
    "def impute_transform(to_impute, to_impute_fit_transform):\n",
    "    temp_df = []\n",
    "    temp_df = to_impute_fit_transform.transform(to_impute)\n",
    "    temp_df = pd.DataFrame(temp_df, columns=to_impute.columns)\n",
    "    to_impute = copy.deepcopy(temp_df)\n",
    "    return to_impute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf30267",
   "metadata": {},
   "source": [
    "<br>  \n",
    "\n",
    "## <span style='color:blue'>Section 4: Function - Oversample and undersample ratios</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e511b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_under_sample_ratios(y, search):\n",
    "    maj_count = Counter(y).most_common()[0][1]\n",
    "    total_ratio = search[0] + search[1]                               # majority and minority\n",
    "    total_count = len(y)\n",
    "    minority_count = np.round(search[1]*total_count/total_ratio, 0)\n",
    "    minority_ratio = minority_count/maj_count\n",
    "    majority_count = np.round(search[0]*total_count/total_ratio, 0)\n",
    "    majority_ratio = minority_count/majority_count\n",
    "    return minority_ratio, majority_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de70086b",
   "metadata": {},
   "source": [
    "<br>  \n",
    "\n",
    "## <span style='color:blue'>Section 5: Function - Oversample and undersample</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2c9b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_sample(X_to_oversample, y_to_oversample, over_sample_sampling_strategy):\n",
    "    o_s = RandomOverSampler(random_state=42, sampling_strategy=over_sample_sampling_strategy)\n",
    "    X_to_oversample, y_to_oversample = o_s.fit_resample(X_to_oversample, y_to_oversample.ravel())\n",
    "    return X_to_oversample, y_to_oversample\n",
    "\n",
    "def under_sample(X_to_undersample, y_to_undersample, under_sample_sampling_strategy):\n",
    "    u_s = RandomUnderSampler(random_state=42, sampling_strategy=under_sample_sampling_strategy)\n",
    "    X_to_undersample, y_to_undersample = u_s.fit_resample(X_to_undersample, y_to_undersample.ravel())\n",
    "    return X_to_undersample, y_to_undersample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a63bb2",
   "metadata": {},
   "source": [
    "<br>  \n",
    "\n",
    "## <span style='color:blue'>Section 6: Function - Scale</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ee72f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_fit_transform(to_scale, to_scale_fit_transform):\n",
    "    temp_df = []\n",
    "    temp_df = to_scale_fit_transform.fit_transform(to_scale)\n",
    "    temp_df = pd.DataFrame(temp_df, columns=to_scale.columns)\n",
    "    to_scale = copy.deepcopy(temp_df)\n",
    "    return to_scale, to_scale_fit_transform\n",
    "\n",
    "def scale_transform(to_scale, to_scale_fit_transform):\n",
    "    temp_df = []\n",
    "    temp_df = to_scale_fit_transform.transform(to_scale)\n",
    "    temp_df = pd.DataFrame(temp_df, columns=to_scale.columns)\n",
    "    to_scale = copy.deepcopy(temp_df)\n",
    "    return to_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe9be0",
   "metadata": {},
   "source": [
    "<br>  \n",
    "\n",
    "## <span style='color:blue'>Section 7: Manual search and cross validate</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe6922",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ---------- set 15-fold cross validation ----------\n",
    "\n",
    "cross_validate = StratifiedKFold(n_splits=15, shuffle=True, random_state=42)\n",
    "\n",
    "# ---------- set parameters and hyperparameters ----------\n",
    "\n",
    "search_parameters = {'majority_minority': [(1,1), (2,1)],\n",
    "                     'decision_boundary': [0.50, 0.35, 0.20]}\n",
    "\n",
    "regular_alpha_from = 0\n",
    "regular_alpha_to = 100\n",
    "regular_alpha_count = 11\n",
    "\n",
    "# ---------- initialise result storages ----------\n",
    "\n",
    "combine_train_validate_accuracy = []\n",
    "combine_train_validate_recall = []\n",
    "combine_train_validate_precision = []\n",
    "\n",
    "train_accuracy = []\n",
    "train_recall = []\n",
    "train_precision = []\n",
    "\n",
    "validate_accuracy = []\n",
    "validate_recall = []\n",
    "validate_precision = []\n",
    "\n",
    "# ---------- loop for minority and majority ----------\n",
    "\n",
    "for i in range(len(search_parameters['majority_minority'])):\n",
    "    \n",
    "    # ---------- loop for decision boundary ----------\n",
    "    \n",
    "    for j in range(len(search_parameters['decision_boundary'])):\n",
    "        \n",
    "        # ---------- loop for regularization strength alpha ----------\n",
    "        \n",
    "        for alpha_log_reg in np.linspace(regular_alpha_from, regular_alpha_to, regular_alpha_count):\n",
    "            \n",
    "            # ---------- loop for cross validation ----------\n",
    "\n",
    "            for train_index, validate_index in cross_validate.split(X_train, y_train):\n",
    "    \n",
    "                # ---------- get train and validate indices ----------\n",
    "    \n",
    "                X_train_train, X_train_validate = X_train.iloc[train_index, :], X_train.iloc[validate_index, :]\n",
    "                y_train_train, y_train_validate = y_train[train_index], y_train[validate_index]\n",
    "                print('Before y_train_train:', Counter(y_train_train))\n",
    "                print('Before y_train_validate:', Counter(y_train_validate))\n",
    "                print('Oversample :', search_parameters['majority_minority'][i])\n",
    "                print('Decision boundary :', search_parameters['decision_boundary'][j])\n",
    "                print('')\n",
    "        \n",
    "                # ---------- impute on train ----------\n",
    "    \n",
    "                knn_impute = KNNImputer(n_neighbors=5)\n",
    "                X_train_train_impute, knn_impute_fit_transform = impute_fit_transform(X_train_train,\n",
    "                                                                                      knn_impute)\n",
    "\n",
    "                # ---------- get oversample and undersample ratios ----------\n",
    "        \n",
    "                over_strategy, under_strategy = over_under_sample_ratios(y_train_train,\n",
    "                                                                         search_parameters['majority_minority'][i])\n",
    "                print('Over :', over_strategy)\n",
    "                print('Under :', under_strategy)\n",
    "        \n",
    "                # ---------- oversample on train ----------\n",
    "    \n",
    "                X_train_train_impute_over, y_train_train_over = over_sample(X_train_train_impute,\n",
    "                                                                            y_train_train,\n",
    "                                                                            over_strategy)\n",
    "                print('Before_Over y_train_train:', Counter(y_train_train_over))\n",
    "                print('')\n",
    "    \n",
    "                # ---------- undersample on train ----------\n",
    "    \n",
    "                X_train_train_impute_over_under, y_train_train_over_under = under_sample(X_train_train_impute_over,\n",
    "                                                                                         y_train_train_over,\n",
    "                                                                                         under_strategy)\n",
    "                print('Before_Over_Under y_train_train:', Counter(y_train_train_over_under))\n",
    "                print('')\n",
    "\n",
    "                # ---------- scale on train ----------\n",
    "    \n",
    "                ss = StandardScaler()\n",
    "                X_train_train_impute_over_under_scale, ss_fit_transform = scale_fit_transform(X_train_train_impute_over_under, ss)\n",
    "    \n",
    "                # ---------- instantiate and fit regularized on train ----------\n",
    "    \n",
    "                X_train_train_impute_over_under_scale = sm.add_constant(X_train_train_impute_over_under_scale)\n",
    "                lr = sm.Logit(y_train_train_over_under, X_train_train_impute_over_under_scale)\n",
    "                lr_result = lr.fit_regularized(maxiter=500, method='l1', alpha=alpha_log_reg)\n",
    "                print('alpha =', alpha_log_reg)\n",
    "                print('')\n",
    "    \n",
    "                # ---------- predict and evaluate on train ----------\n",
    "    \n",
    "                y_train_train_over_under_predicted = (lr_result.predict(X_train_train_impute_over_under_scale) >= search_parameters['decision_boundary'][j]).astype(int)\n",
    "        \n",
    "                train_accuracy.append(accuracy_score(y_train_train_over_under, y_train_train_over_under_predicted))\n",
    "                train_recall.append(recall_score(y_train_train_over_under, y_train_train_over_under_predicted))\n",
    "                train_precision.append(precision_score(y_train_train_over_under, y_train_train_over_under_predicted))\n",
    "    \n",
    "                # ---------- impute on validate ----------\n",
    "    \n",
    "                X_train_validate_impute = impute_transform(X_train_validate, knn_impute_fit_transform)\n",
    "    \n",
    "                # ---------- scale on validate ----------\n",
    "    \n",
    "                X_train_validate_impute_scale = scale_transform(X_train_validate_impute, ss_fit_transform)\n",
    "    \n",
    "                # ---------- predict and evaluate on validate ----------\n",
    "    \n",
    "                X_train_validate_impute_scale = sm.add_constant(X_train_validate_impute_scale)\n",
    "                y_train_validate_predicted = (lr_result.predict(X_train_validate_impute_scale) >= search_parameters['decision_boundary'][j]).astype(int)\n",
    "        \n",
    "                validate_accuracy.append(accuracy_score(y_train_validate, y_train_validate_predicted))\n",
    "                validate_recall.append(recall_score(y_train_validate, y_train_validate_predicted))\n",
    "                validate_precision.append(precision_score(y_train_validate, y_train_validate_predicted))\n",
    "    \n",
    "            combine_train_validate_accuracy.append([search_parameters['majority_minority'][i],\n",
    "                                                    search_parameters['decision_boundary'][j],\n",
    "                                                    alpha_log_reg,\n",
    "                                                    np.mean(train_accuracy),\n",
    "                                                    np.std(train_accuracy),\n",
    "                                                    np.mean(validate_accuracy),\n",
    "                                                    np.std(validate_accuracy)])\n",
    "            combine_train_validate_recall.append([search_parameters['majority_minority'][i],\n",
    "                                                  search_parameters['decision_boundary'][j],\n",
    "                                                  alpha_log_reg,\n",
    "                                                  np.mean(train_recall),\n",
    "                                                  np.std(train_recall),\n",
    "                                                  np.mean(validate_recall),\n",
    "                                                  np.std(validate_recall)])\n",
    "            combine_train_validate_precision.append([search_parameters['majority_minority'][i],\n",
    "                                                     search_parameters['decision_boundary'][j],\n",
    "                                                     alpha_log_reg,\n",
    "                                                     np.mean(train_precision),\n",
    "                                                     np.std(train_precision),\n",
    "                                                     np.mean(validate_precision),\n",
    "                                                     np.std(validate_precision)])\n",
    "            \n",
    "            train_accuracy = []\n",
    "            train_recall = []\n",
    "            train_precision = []\n",
    "\n",
    "            validate_accuracy = []\n",
    "            validate_recall = []\n",
    "            validate_precision = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cfcbaa",
   "metadata": {},
   "source": [
    "<br>  \n",
    "\n",
    "## <span style='color:blue'>Section 8: Save results</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a24707",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_train_validate_accuracy = pd.DataFrame(combine_train_validate_accuracy,\n",
    "                                               columns=['majority_minority',\n",
    "                                                        'decision_boundary',\n",
    "                                                        'alpha',\n",
    "                                                        'train_accuracy_mean',\n",
    "                                                        'train_accuracy_std_dev',\n",
    "                                                        'validate_accuracy_mean',\n",
    "                                                        'validate_accuracy_std_dev'])\n",
    "combine_train_validate_accuracy['overfit'] = (combine_train_validate_accuracy['validate_accuracy_mean']-combine_train_validate_accuracy['train_accuracy_mean'])/combine_train_validate_accuracy['train_accuracy_mean']*100\n",
    "combine_train_validate_accuracy.to_csv('../data/tried_combine_train_validate_accuracy.csv', na_rep='NaN', index_label='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801e948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_train_validate_recall = pd.DataFrame(combine_train_validate_recall,\n",
    "                                             columns=['majority_minority',\n",
    "                                                      'decision_boundary',\n",
    "                                                      'alpha',\n",
    "                                                      'train_recall_mean',\n",
    "                                                      'train_recall_std_dev',\n",
    "                                                      'validate_recall_mean',\n",
    "                                                      'validate_recall_std_dev'])\n",
    "combine_train_validate_recall['overfit'] = (combine_train_validate_recall['validate_recall_mean']-combine_train_validate_recall['train_recall_mean'])/combine_train_validate_recall['train_recall_mean']*100\n",
    "combine_train_validate_recall.to_csv('../data/tried_combine_train_validate_recall.csv', na_rep='NaN', index_label='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a3f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_train_validate_precision = pd.DataFrame(combine_train_validate_precision,\n",
    "                                                columns=['majority_minority',\n",
    "                                                         'decision_boundary',\n",
    "                                                         'alpha',\n",
    "                                                         'train_precision_mean',\n",
    "                                                         'train_precision_std_dev',\n",
    "                                                         'validate_precision_mean',\n",
    "                                                         'validate_precision_std_dev'])\n",
    "combine_train_validate_precision['overfit'] = (combine_train_validate_precision['validate_precision_mean']-combine_train_validate_precision['train_precision_mean'])/combine_train_validate_precision['train_precision_mean']*100\n",
    "combine_train_validate_precision.to_csv('../data/tried_combine_train_validate_precision.csv', na_rep='NaN', index_label='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4835e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
